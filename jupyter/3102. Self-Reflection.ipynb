{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Reflection 통한 First-Stage Retrieve 성능 개선\n",
    "* 기존의 basic + custom 데이터셋을 통해 최적화한 결과, 목표 성능(recall: 0.8)에 도달\n",
    "    * k: 20, alpha: 40, morphological_analyzer: bm25_kiki_pos, score_threshold: 0.1\n",
    "    * 위의 조합에서 recall: 0.815476 달성\n",
    "* rag 시스템 연구를 위해 k: 10, alpha: 40, morphological_analyzer: bm25_kiki_pos, score_threshold: 0.1 조합을 기준으로 Self-Reflection 관련 실험 진행\n",
    "* 아래의 5 가지 self-reflection을 대상으로 first-stage retrieve의 성능 개선 실험\n",
    "    * retrieved contexts + input query 대상\n",
    "        * Binary Classification\n",
    "        * Confidence Score\n",
    "        * CoT + Few-Shot Confidence Score\n",
    "    * generated answer + input query 대상\n",
    "        * Binary Classification\n",
    "        * Confidence Score\n",
    "\n",
    "### 결과\n",
    "* retrieved contexts + input query 기반 grader는 generated answer + input query 기반 grader보다 성능이 떨어짐.\n",
    "    * retrieved contexts는 10개 * 500 토큰 약 5000 토큰 이상의 텍스트를 입력받음\n",
    "    * 문서 평가 및 종합 과정에서 과도한 정보량으로 인해 정확한 예측이 어려움\n",
    "* Binary classification과 Confidence Score 비교\n",
    "    * Confidence Score와 threshold 값을 통해 Binary classification보다 좋은 성능의 로직 구축을 희망했으나 목표 달성 실패\n",
    "    * Binary Classification이 더 안정적이고 높은 성능(precision: 0.70-0.76, f1-score: 0.73)을 보임\n",
    "* Self-Reflection을 통한 First-Stage Retrieve 성능 개선은 가능하나, 구현 복잡도와 비용 대비 효과가 제한적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../code/graphParser')\n",
    "sys.path.append('../code/ragas_custom')\n",
    "from rateLimit import handle_rate_limits\n",
    "from retrieve.sparse import BM25\n",
    "from retrieve.config import generate_retriever_configs\n",
    "from evaluation.retrieve import optimization, combine_hybrid_results\n",
    "\n",
    "from langchain_core.prompts import load_prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "kg = KnowledgeGraph.load('../data/rag/kg.json')\n",
    "\n",
    "documents = [Document(page_content=node.properties['page_content'],\n",
    "                      metadata=node.properties['document_metadata'])\n",
    "                       for node in kg.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_config = {\n",
    "    'k': 10,\n",
    "    'alpha': 40,\n",
    "    'dense_type': 'threshold',\n",
    "    'dense_params': {'score_threshold': 0.1},\n",
    "    'morphological_analyzer': 'bm25_kiwi_pos'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi_pos = BM25(k=15, type='kiwi_pos')\n",
    "texts = [node.properties['page_content'] for node in kg.nodes]\n",
    "kiwi_pos.from_texts(texts)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = pd.read_csv('../data/rag/valid_dataset.csv')\n",
    "merged_dataset['reference_contexts'] = merged_dataset['reference_contexts'].apply(lambda x : eval(x))\n",
    "\n",
    "for column in merged_dataset.columns[5:]:\n",
    "    merged_dataset[column] = merged_dataset[column].apply(lambda x : eval(x))\n",
    "\n",
    "merged_dataset['precompute_dense'] = merged_dataset['user_input'].apply(lambda x : db.similarity_search_with_score(x, k=int(15)))\n",
    "merged_dataset['precompute_sparse_bm25_kiwi_pos'] = merged_dataset['user_input'].apply(lambda x : kiwi_pos.search(x))\n",
    "\n",
    "merged_dataset['precompute_dense'] = merged_dataset['precompute_dense'].apply(lambda results: [doc.page_content for result in results for doc, score in [result] if score >= standard_config['dense_params']['score_threshold']])\n",
    "merged_dataset['result_retrieve'] = combine_hybrid_results(merged_dataset['precompute_dense'], merged_dataset['precompute_sparse_bm25_kiwi_pos'], standard_config['alpha'], standard_config['k'])\n",
    "merged_dataset['need_retrieve'] = merged_dataset.apply(lambda x : 'no' if len(set(x['reference_contexts']).intersection(set(x['result_retrieve']))) == len(x['reference_contexts']) else 'yes', axis=1)\n",
    "\n",
    "merged_dataset = merged_dataset[merged_dataset.columns[:5].to_list() + ['result_retrieve', 'need_retrieve']]\n",
    "# merged_dataset.to_csv('../data/rag/self_reflection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_sufficiency(df: pd.DataFrame, column: str):\n",
    "    df = df.copy()\n",
    "    df['prediction'] = df[column].apply(lambda x : x[0].lower())\n",
    "\n",
    "    df['label'] = df['need_retrieve'].str.lower()\n",
    "\n",
    "    y_true = df['label']\n",
    "    y_pred = df['prediction']\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, pos_label='yes'),\n",
    "        \"recall\": recall_score(y_true, y_pred, pos_label='yes'),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, pos_label='yes'),\n",
    "    }\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"no\", \"yes\"])\n",
    "\n",
    "    return metrics, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence_thresholds(df, score_col='score', label_col='need_retrieve', thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.0, 1.01, 0.1)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    df[score_col] = df[score_col].apply(lambda x : x[0])\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        df['pred'] = df[score_col] >= thresh\n",
    "\n",
    "        y_true = df[label_col].map({'yes': 1, 'no': 0})\n",
    "        y_pred = df['pred'].astype(int)\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        total = len(df)\n",
    "        predicted_positive = y_pred.sum()\n",
    "        retrieve_rate = predicted_positive / total\n",
    "\n",
    "        # 실제 필요했던 retrieve 수\n",
    "        true_positive = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "        false_positive = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "        actual_positive = y_true.sum()\n",
    "\n",
    "        results.append({\n",
    "            'threshold': thresh,\n",
    "            'precision': round(precision, 3),\n",
    "            'recall': round(recall, 3),\n",
    "            'f1_score': round(f1, 3),\n",
    "            'retrieve_count': predicted_positive,\n",
    "            'retrieve_rate': round(retrieve_rate, 3),\n",
    "            'true_positive': true_positive,\n",
    "            'false_positive': false_positive,\n",
    "            'missed_retrieves': actual_positive - true_positive\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieved Contexts 대상 Grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @handle_rate_limits\n",
    "def binaryClassification_chain(data_batches, model_name='gpt-4o-mini', current_api_key=None, max_concurrency=5) -> List:\n",
    "    class BinaryClassificationOutput(BaseModel):\n",
    "        binary_classification: str = Field(..., description=\"Binary Response, 'Yes' or 'No'\")\n",
    "        reasoning: str = Field(..., description=\"Maximum 3 sentences explaining the decision\")\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=BinaryClassificationOutput)\n",
    "    prompt = load_prompt('../prompt/reflection/binaryClassification.yaml')\n",
    "    prompt = prompt.partial(format=parser.get_format_instructions())\n",
    "    # llm = ChatOpenAI(model=model_name, temperature=0.2, api_key=current_api_key)\n",
    "    llm = OllamaLLM(model='gemma3:12b')\n",
    "    \n",
    "    binaryClassification_chain = prompt | llm\n",
    "\n",
    "    results = binaryClassification_chain.batch(data_batches, config={\"max_concurrency\": max_concurrency})\n",
    "    results = [(parser.parse(result).binary_classification, parser.parse(result).reasoning) for result in results]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [06:30<00:00, 13.94s/it]\n"
     ]
    }
   ],
   "source": [
    "max_concurrency = 2\n",
    "input_querys = merged_dataset['user_input'].to_list()\n",
    "documents = merged_dataset['result_retrieve'].apply(lambda x : '\\n\\n'.join([f\"[Document {i+1}]\\n{doc.strip()}\" for i, doc in enumerate(x)])).to_list()\n",
    "input_data = [{'document': document, 'question': question} for document, question in zip(documents, input_querys)]\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(len(input_data) // max_concurrency)):\n",
    "    results.extend(binaryClassification_chain(input_data[i * max_concurrency: (i + 1) * max_concurrency], max_concurrency=max_concurrency))\n",
    "\n",
    "if len(input_data) % max_concurrency != 0:\n",
    "    results.extend(binaryClassification_chain(input_data[len(input_data) // max_concurrency * max_concurrency:], max_concurrency=max_concurrency))\n",
    "\n",
    "merged_dataset['binary_classification'] = results\n",
    "merged_dataset.to_csv('../data/rag/self_reflection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>reference_contexts_section</th>\n",
       "      <th>result_retrieve</th>\n",
       "      <th>need_retrieve</th>\n",
       "      <th>binary_classification</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>generated_need_retrieve</th>\n",
       "      <th>generated_confidence_need_retrieve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>바벨 잡는 방법과 앉아받기 동작의 올바른 수행을 결합하여 최적의 리프팅 기술을 설명해줘.</td>\n",
       "      <td>[바벨을 잡는 방법에는 크게 오버그립(over grip), 언더그립(under gr...</td>\n",
       "      <td>바벨 잡는 방법에는 오버그립, 언더그립, 리버스그립, 훅그립이 있으며, 각각의 그립...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "      <td>['Ⅲ. 역도경기 기술의 구조와 훈련법', 'Ⅲ. 역도경기 기술의 구조와 훈련법']</td>\n",
       "      <td>['순간적으로 무거운 물체를 들어 올리는 데에는 근력 외에도 강인한 정신력이 요구\\...</td>\n",
       "      <td>yes</td>\n",
       "      <td>(Yes, Combining a proper barbell grip with a c...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>바벨을 잡는 방법과 앉아받기 동작의 올바른 수행을 결합하여 최적의 리프팅 기술을 설...</td>\n",
       "      <td>No</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>스포츠 심리학자들은 선수의 성과 향상에 어떤 역할을 하며, 그들의 개인 문제에 대한...</td>\n",
       "      <td>[성공적인 상\\n담 진행을 위해서 상담사는 내담자의 감정에 공감할 수 있어야 한다....</td>\n",
       "      <td>스포츠 심리학자들은 운동선수의 성과를 향상시키기 위해 여러 가지 역할을 수행합니다....</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "      <td>['II. 역도의 스포츠 과학적 원리', 'II. 역도의 스포츠 과학적 원리']</td>\n",
       "      <td>['스포츠심리학을 연구하고 스포츠심리학의 연구 성과를 스포츠 현장에 적용하는\\n스포...</td>\n",
       "      <td>no</td>\n",
       "      <td>(Yes, Sports psychologists play a vital role i...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>스포츠 심리학자들은 선수의 성과 향상에 중요한 역할을 하며, 선수의 개인 문제에 대...</td>\n",
       "      <td>No</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  바벨 잡는 방법과 앉아받기 동작의 올바른 수행을 결합하여 최적의 리프팅 기술을 설명해줘.   \n",
       "1  스포츠 심리학자들은 선수의 성과 향상에 어떤 역할을 하며, 그들의 개인 문제에 대한...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [바벨을 잡는 방법에는 크게 오버그립(over grip), 언더그립(under gr...   \n",
       "1  [성공적인 상\\n담 진행을 위해서 상담사는 내담자의 감정에 공감할 수 있어야 한다....   \n",
       "\n",
       "                                           reference  \\\n",
       "0  바벨 잡는 방법에는 오버그립, 언더그립, 리버스그립, 훅그립이 있으며, 각각의 그립...   \n",
       "1  스포츠 심리학자들은 운동선수의 성과를 향상시키기 위해 여러 가지 역할을 수행합니다....   \n",
       "\n",
       "                       synthesizer_name  \\\n",
       "0  multi_hop_abstract_query_synthesizer   \n",
       "1  multi_hop_abstract_query_synthesizer   \n",
       "\n",
       "                       reference_contexts_section  \\\n",
       "0  ['Ⅲ. 역도경기 기술의 구조와 훈련법', 'Ⅲ. 역도경기 기술의 구조와 훈련법']   \n",
       "1    ['II. 역도의 스포츠 과학적 원리', 'II. 역도의 스포츠 과학적 원리']   \n",
       "\n",
       "                                     result_retrieve need_retrieve  \\\n",
       "0  ['순간적으로 무거운 물체를 들어 올리는 데에는 근력 외에도 강인한 정신력이 요구\\...           yes   \n",
       "1  ['스포츠심리학을 연구하고 스포츠심리학의 연구 성과를 스포츠 현장에 적용하는\\n스포...            no   \n",
       "\n",
       "                               binary_classification  confidence_score  \\\n",
       "0  (Yes, Combining a proper barbell grip with a c...              0.85   \n",
       "1  (Yes, Sports psychologists play a vital role i...              0.85   \n",
       "\n",
       "                                    generated_answer generated_need_retrieve  \\\n",
       "0  바벨을 잡는 방법과 앉아받기 동작의 올바른 수행을 결합하여 최적의 리프팅 기술을 설...                      No   \n",
       "1  스포츠 심리학자들은 선수의 성과 향상에 중요한 역할을 하며, 선수의 개인 문제에 대...                      No   \n",
       "\n",
       "   generated_confidence_need_retrieve  \n",
       "0                                 0.7  \n",
       "1                                 0.6  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.4642857142857143, 'precision': 0.5, 'recall': 0.7666666666666667, 'f1_score': 0.6052631578947368}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.30      0.12      0.17        26\n",
      "         yes       0.50      0.77      0.61        30\n",
      "\n",
      "    accuracy                           0.46        56\n",
      "   macro avg       0.40      0.44      0.39        56\n",
      "weighted avg       0.41      0.46      0.40        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics, report = evaluate_binary_sufficiency(merged_dataset, 'binary_classification')\n",
    "print(metrics)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @handle_rate_limits\n",
    "def confidenceScore_chain(data_batches, model_name='gpt-4o-mini', current_api_key=None, max_concurrency=5) -> List:\n",
    "    class ConfidenceScoreOutput(BaseModel):\n",
    "        confidence_score: float = Field(..., description=\"Confidence Score, 0.00 to 1.00\")\n",
    "        reasoning: str = Field(..., description=\"Maximum 3 sentences explaining the decision\")\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=ConfidenceScoreOutput)\n",
    "    prompt = load_prompt('../prompt/reflection/confidenceScore.yaml')\n",
    "    prompt = prompt.partial(format=parser.get_format_instructions())\n",
    "    # llm = ChatOpenAI(model=model_name, temperature=0.2, api_key=current_api_key)\n",
    "    llm = OllamaLLM(model='gemma3:12b')\n",
    "    \n",
    "    confidenceScore_chain = prompt | llm\n",
    "\n",
    "    results = confidenceScore_chain.batch(data_batches, config={\"max_concurrency\": max_concurrency})\n",
    "    results = [(parser.parse(result).confidence_score, parser.parse(result).reasoning) for result in results]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [06:28<00:00, 21.57s/it]\n"
     ]
    }
   ],
   "source": [
    "max_concurrency = 3\n",
    "input_querys = merged_dataset['user_input'].to_list()\n",
    "documents = merged_dataset['result_retrieve'].apply(lambda x : '\\n\\n'.join([f\"[Document {i+1}]\\n{doc.strip()}\" for i, doc in enumerate(x)])).to_list()\n",
    "input_data = [{'document': document, 'question': question} for document, question in zip(documents, input_querys)]\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(len(input_data) // max_concurrency)):\n",
    "    results.extend(confidenceScore_chain(input_data[i * max_concurrency: (i + 1) * max_concurrency], max_concurrency=max_concurrency))\n",
    "\n",
    "if len(input_data) % max_concurrency != 0:\n",
    "    results.extend(confidenceScore_chain(input_data[len(input_data) // max_concurrency * max_concurrency:], max_concurrency=max_concurrency))\n",
    "\n",
    "merged_dataset['confidence_score'] = results\n",
    "merged_dataset.to_csv('../data/rag/self_reflection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>need_retrieve</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no</td>\n",
       "      <td>0.95</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.95</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  need_retrieve  confidence_score  count\n",
       "0            no              0.70      3\n",
       "1            no              0.75      3\n",
       "2            no              0.85      3\n",
       "3            no              0.90      2\n",
       "4            no              0.95     15\n",
       "5           yes              0.70      1\n",
       "6           yes              0.75      5\n",
       "7           yes              0.85      2\n",
       "8           yes              0.95     22"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = merged_dataset.copy()\n",
    "\n",
    "tmp['confidence_score'] = tmp['confidence_score'].apply(lambda x : x[0])\n",
    "tmp.groupby(['need_retrieve', 'confidence_score']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. CoT + Few-Shot Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @handle_rate_limits\n",
    "def gradeDocument_chain(data_batches, model_name='gpt-4o-mini', current_api_key=None, max_concurrency=5) -> List:\n",
    "    class GradeDocument(BaseModel):\n",
    "        \"\"\"Score from 0.00 to 1.00 indicating how necessary it is to re-retrieve documents to answer the question.\"\"\"\n",
    "        re_retrieve_score: float = Field(\n",
    "            description=\"A score between 0.0 (no re-retrieval needed) and 1.0 (re-retrieval strongly needed)\"\n",
    "        )\n",
    "        reasoning: str = Field(..., description=\"Maximum 3 sentences explaining the decision\")\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=GradeDocument)\n",
    "    prompt = load_prompt('../prompt/reflection/retrieveGrader.yaml')\n",
    "    prompt = prompt.partial(format=parser.get_format_instructions())\n",
    "\n",
    "    # llm = ChatOpenAI(model=model_name, temperature=0.2)\n",
    "    llm = OllamaLLM(model='gemma3:12b')\n",
    "    gradeDocuments_chain = prompt | llm\n",
    "\n",
    "    results = gradeDocuments_chain.batch(data_batches, config={\"max_concurrency\": max_concurrency})\n",
    "    results = [(parser.parse(result).re_retrieve_score, parser.parse(result).reasoning) for result in results]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_concurrency = 3\n",
    "input_querys = merged_dataset['user_input'].to_list()\n",
    "documents = merged_dataset['result_retrieve'].apply(lambda x : '\\n\\n'.join([f\"[Document {i+1}]\\n{doc.strip()}\" for i, doc in enumerate(x)])).to_list()\n",
    "\n",
    "input_data = [{'document': document, 'question': question} for document, question in zip(documents, input_querys)]\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(len(input_data) // max_concurrency)):\n",
    "    results.extend(gradeDocument_chain(input_data[i * max_concurrency: (i + 1) * max_concurrency], max_concurrency=max_concurrency))\n",
    "\n",
    "if len(input_data) % max_concurrency != 0:\n",
    "    results.extend(gradeDocument_chain(input_data[len(input_data) // max_concurrency * max_concurrency:], max_concurrency=max_concurrency))\n",
    "\n",
    "result_df = merged_dataset.copy()\n",
    "result_df = result_df[result_df.columns[:5].to_list() + ['need_retrieve']]\n",
    "result_df['gradeDocument'] = results\n",
    "\n",
    "merged_dataset.to_csv('../data/rag/self_reflection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_confidence_thresholds(result_df, 'gradeDocument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>need_retrieve</th>\n",
       "      <th>gradeDocument</th>\n",
       "      <th>user_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>0.80</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.80</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  need_retrieve  gradeDocument  user_input\n",
       "0            no           0.75           8\n",
       "1            no           0.80          18\n",
       "2           yes           0.75           8\n",
       "3           yes           0.80          19\n",
       "4           yes           1.00           3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.groupby(['need_retrieve', 'gradeDocument'])['user_input'].count().reset_index()                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generated answer 대상 Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = pd.read_csv('../data/rag/self_reflection.csv')\n",
    "merged_dataset['reference_contexts'] = merged_dataset['reference_contexts'].apply(lambda x : eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@handle_rate_limits\n",
    "def generate_chain(data_batches, model_name='gpt-4o-mini', current_api_key=None, max_concurrency=5) -> List:\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Answer in Korean.\n",
    "\n",
    "    #Question: \n",
    "    {question} \n",
    "    #Context: \n",
    "    {context} \n",
    "\n",
    "    #Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    # llm = OllamaLLM(model='gemma3:12b')\n",
    "    llm = ChatOpenAI(model=model_name, temperature=0.2, api_key=current_api_key)\n",
    "    generate_chain = prompt | llm\n",
    "\n",
    "    results = generate_chain.batch(data_batches, config={\"max_concurrency\": max_concurrency})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_12096\\1119896850.py:18: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=model_name, temperature=0.2, api_key=current_api_key)\n",
      "100%|██████████| 28/28 [02:27<00:00,  5.25s/it]\n"
     ]
    }
   ],
   "source": [
    "max_concurrency = 2\n",
    "input_querys = merged_dataset['user_input'].to_list()\n",
    "documents = merged_dataset['result_retrieve'].apply(lambda x : '\\n\\n'.join([doc for doc in x])).to_list()\n",
    "input_data = [{'context': document, 'question': question} for document, question in zip(documents, input_querys)]\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(len(input_data) // max_concurrency)):\n",
    "    results.extend(generate_chain(input_data[i * max_concurrency: (i + 1) * max_concurrency], max_concurrency=max_concurrency))\n",
    "\n",
    "if len(input_data) % max_concurrency != 0:\n",
    "    results.extend(generate_chain(input_data[len(input_data) // max_concurrency * max_concurrency:], max_concurrency=max_concurrency))\n",
    "\n",
    "results = [result.content for result in results]\n",
    "merged_dataset['generated_answer'] = results\n",
    "merged_dataset.to_csv('../data/rag/self_reflection.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @handle_rate_limits\n",
    "def generate_binary_chain(data_batches, model_name='gpt-4o-mini', current_api_key=None, max_concurrency=5) -> List:\n",
    "    class NeedRetrieve(BaseModel):\n",
    "        need_retrieve: str = Field(description=\"Whether the retrieval step in a retrieval-augmented generation (RAG) system should be re-executed to improve the answer quality. Respond with 'Yes' or 'No'.\")\n",
    "        reasoning: str = Field(description=\"Maximum 3 sentences explaining the decision\")\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=NeedRetrieve)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are tasked with evaluating whether the retrieval step in a retrieval-augmented generation (RAG) system should be re-executed to improve the generated answer.\n",
    "\n",
    "        Consider the following factors:\n",
    "        1. Does the generated answer fully and directly address the user's intent?\n",
    "        2. Are there important missing details, factual inaccuracies, or hallucinations?\n",
    "        3. Would retrieving better or more specific documents likely improve the response quality?\n",
    "\n",
    "        Follow this process:\n",
    "        - First, provide a brief reasoning explaining your judgment in korean. Be specific: refer to any missing or inadequate parts of the answer.\n",
    "        - Then, decide whether retrieval should be redone by responding with **\"Yes\"** or **\"No\"**.\n",
    "\n",
    "        Instructions:\n",
    "        - Respond with \"Yes\" if redoing retrieval is likely to produce a significantly better answer.\n",
    "        - Respond with \"No\" if the current answer is sufficient and retrieval would not meaningfully improve it.\n",
    "\n",
    "        ---\n",
    "\n",
    "        User Input:\n",
    "        {user_input}\n",
    "\n",
    "        Generated Answer:\n",
    "        {generated_answer}\n",
    "\n",
    "        ---\n",
    "\n",
    "        Reasoning:\n",
    "        (Write your explanation here.)\n",
    "\n",
    "        Final decision (Yes/No):\n",
    "\n",
    "        format:\n",
    "        {format}\n",
    "        \"\"\")\n",
    "\n",
    "    prompt = prompt.partial(format=parser.get_format_instructions())\n",
    "\n",
    "    llm = OllamaLLM(model='gemma3:12b')\n",
    "    # llm = ChatOpenAI(model=model_name, temperature=0.2, api_key=current_api_key)\n",
    "    generate_binary_chain = prompt | llm\n",
    "\n",
    "    results = generate_binary_chain.batch(data_batches, config={\"max_concurrency\": max_concurrency})\n",
    "    results = [(parser.parse(result).need_retrieve, parser.parse(result).reasoning) for result in results]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [04:25<00:00, 14.76s/it]\n"
     ]
    }
   ],
   "source": [
    "max_concurrency = 3\n",
    "input_querys = merged_dataset['user_input'].to_list()\n",
    "generated_answers = merged_dataset['generated_answer'].to_list()\n",
    "input_data = [{'user_input': question, 'generated_answer': answer} for question, answer in zip(input_querys, generated_answers)]\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(len(input_data) // max_concurrency)):\n",
    "    results.extend(generate_binary_chain(input_data[i * max_concurrency: (i + 1) * max_concurrency], max_concurrency=max_concurrency))\n",
    "\n",
    "if len(input_data) % max_concurrency != 0:\n",
    "    results.extend(generate_binary_chain(input_data[len(input_data) // max_concurrency * max_concurrency:], max_concurrency=max_concurrency))\n",
    "\n",
    "merged_dataset['generated_need_retrieve'] = results\n",
    "merged_dataset.to_csv('../data/rag/self_reflection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7321428571428571, 'precision': 0.7586206896551724, 'recall': 0.7333333333333333, 'f1_score': 0.7457627118644068}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.70      0.73      0.72        26\n",
      "         yes       0.76      0.73      0.75        30\n",
      "\n",
      "    accuracy                           0.73        56\n",
      "   macro avg       0.73      0.73      0.73        56\n",
      "weighted avg       0.73      0.73      0.73        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics, report = evaluate_binary_sufficiency(merged_dataset, 'generated_need_retrieve')\n",
    "print(metrics)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @handle_rate_limits\n",
    "def generate_confidence_chain(data_batches, model_name='gpt-4o-mini', current_api_key=None, max_concurrency=5) -> List:\n",
    "    class NeedRetrieve(BaseModel):\n",
    "        retrieval_necessity: float = Field(\n",
    "            description=\"A confidence score between 0.0 and 1.0 indicating whether the retrieval step should be re-executed. 0.0 = no need, 1.0 = strongly needs to be re-executed.\"\n",
    "        )\n",
    "        reasoning: str = Field(description=\"Maximum 3 sentences explaining the decision\")\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=NeedRetrieve)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "        Given the user's input and the generated answer, evaluate how strongly the retrieval step in a\n",
    "        retrieval-augmented generation (RAG) system should be re-executed to improve the answer quality.\n",
    "        Output a confidence score between 0.0 and 1.0.\n",
    "        before you output the score, reason about the factors that influence the score in korean.\n",
    "        ---\n",
    "\n",
    "        Consider the following factors:\n",
    "\n",
    "        - How well the generated answer addresses the user's intent (0.0 = completely irrelevant, 1.0 = fully aligned)\n",
    "        - Whether the answer contains hallucinations, missing key details, or irrelevant information\n",
    "        - Whether better or more specific supporting documents could significantly improve the response\n",
    "\n",
    "        User Input:\n",
    "        {user_input}\n",
    "\n",
    "        Generated Answer:\n",
    "        {generated_answer}\n",
    "\n",
    "        format:\n",
    "        {format}\n",
    "        \"\"\"\n",
    "    )\n",
    "    prompt = prompt.partial(format=parser.get_format_instructions())\n",
    "\n",
    "    llm = OllamaLLM(model='gemma3:12b')\n",
    "    generate_confidence_chain = prompt | llm\n",
    "\n",
    "    results = generate_confidence_chain.batch(data_batches, config={\"max_concurrency\": max_concurrency})\n",
    "    results = [(parser.parse(result).retrieval_necessity, parser.parse(result).reasoning) for result in results]\n",
    "\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [05:32<00:00, 18.46s/it]\n"
     ]
    }
   ],
   "source": [
    "max_concurrency = 3\n",
    "input_querys = merged_dataset['user_input'].to_list()\n",
    "generated_answers = merged_dataset['generated_answer'].to_list()\n",
    "input_data = [{'user_input': question, 'generated_answer': answer} for question, answer in zip(input_querys, generated_answers)]\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(len(input_data) // max_concurrency)):\n",
    "    results.extend(generate_confidence_chain(input_data[i * max_concurrency: (i + 1) * max_concurrency], max_concurrency=max_concurrency))\n",
    "\n",
    "if len(input_data) % max_concurrency != 0:\n",
    "    results.extend(generate_confidence_chain(input_data[len(input_data) // max_concurrency * max_concurrency:], max_concurrency=max_concurrency))\n",
    "\n",
    "merged_dataset['generated_confidence_need_retrieve'] = results\n",
    "merged_dataset.to_csv('../data/rag/self_reflection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>retrieve_count</th>\n",
       "      <th>retrieve_rate</th>\n",
       "      <th>true_positive</th>\n",
       "      <th>false_positive</th>\n",
       "      <th>missed_retrieves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.698</td>\n",
       "      <td>56</td>\n",
       "      <td>1.000</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.536</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.698</td>\n",
       "      <td>56</td>\n",
       "      <td>1.000</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.536</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.698</td>\n",
       "      <td>56</td>\n",
       "      <td>1.000</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.656</td>\n",
       "      <td>34</td>\n",
       "      <td>0.607</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.656</td>\n",
       "      <td>34</td>\n",
       "      <td>0.607</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.656</td>\n",
       "      <td>34</td>\n",
       "      <td>0.607</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.065</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold  precision  recall  f1_score  retrieve_count  retrieve_rate  \\\n",
       "0         0.0      0.536   1.000     0.698              56          1.000   \n",
       "1         0.1      0.536   1.000     0.698              56          1.000   \n",
       "2         0.2      0.536   1.000     0.698              56          1.000   \n",
       "3         0.3      0.618   0.700     0.656              34          0.607   \n",
       "4         0.4      0.618   0.700     0.656              34          0.607   \n",
       "5         0.5      0.618   0.700     0.656              34          0.607   \n",
       "6         0.6      1.000   0.033     0.065               1          0.018   \n",
       "7         0.7      0.000   0.000     0.000               0          0.000   \n",
       "8         0.8      0.000   0.000     0.000               0          0.000   \n",
       "9         0.9      0.000   0.000     0.000               0          0.000   \n",
       "10        1.0      0.000   0.000     0.000               0          0.000   \n",
       "\n",
       "    true_positive  false_positive  missed_retrieves  \n",
       "0              30              26                 0  \n",
       "1              30              26                 0  \n",
       "2              30              26                 0  \n",
       "3              21              13                 9  \n",
       "4              21              13                 9  \n",
       "5              21              13                 9  \n",
       "6               1               0                29  \n",
       "7               0               0                30  \n",
       "8               0               0                30  \n",
       "9               0               0                30  \n",
       "10              0               0                30  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_confidence_thresholds(merged_dataset, 'generated_confidence_need_retrieve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>need_retrieve</th>\n",
       "      <th>generated_confidence_need_retrieve</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>0.6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  need_retrieve  generated_confidence_need_retrieve  count\n",
       "0            no                                 0.3     13\n",
       "1            no                                 0.6     13\n",
       "2           yes                                 0.3      9\n",
       "3           yes                                 0.6     20\n",
       "4           yes                                 0.7      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = merged_dataset.copy()\n",
    "# tmp['generated_confidence_need_retrieve'] = tmp['generated_confidence_need_retrieve'].apply(lambda x : x[0])\n",
    "\n",
    "tmp.groupby(['need_retrieve', 'generated_confidence_need_retrieve']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SportAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
