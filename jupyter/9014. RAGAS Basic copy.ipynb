{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Customization의 필요성\n",
    "## 1 실제 사용자 질의와 ragas 평가용 질의의 간극 해소\n",
    "* ragas의 기본 데이터 생성 방식은 대부분의 문서에 적용 가능한 범용적인 문서-질의-응답 패턴을 기반으로 설계됨\n",
    "  * 문서를 통한 페르소나 생성이 아닌 실제 사용자 페르소나 입력을 통해 실제와 가까운 질의 생성\n",
    "* summary embedding의 유사도를 중심으로 문서를 선택하고 질의를 생성하는 구조적 특성으로 인해 다음과 같은 문제가 발생\n",
    "  * Multi-Hop 평가 데이터 생성에서 페르소나 생성이나 문서 선택 단계에서 summary embedding을 주로 활용\n",
    "  * 이로 인해, 동일 키워드에 대한 서로 다른 주제나 테마를 가진 문서들 기반 평가 데이터셋 생성 불가능\n",
    "  * summary embedding의 값이 유사성으로 인해 single-section의 문서 조합 기반 평가 데이터셋이 주로 나타남남\n",
    "  * 결과적으로 실제 사용자 질의의 복합적 의도나 문맥적 요구사항을 충분히 반영하지 못하는 한계 발생\n",
    "  * 질의 예시\n",
    "    * ex) \"**sumo 데드리프트의 마무리 동작에서 허리**에 과도한 무게 집중이 나타나고 있어. 이 현상이 나타나는 **과학적 이유**와 이를 해결하기 위한 **연습 방법**을 작성해줘.\"\n",
    "      * 중심 키워드: sumo 데드리프트, 마무리 동작, 허리\n",
    "      * 주제: 과학적 이유, 연습 방법\n",
    "    * 이와 같이, 실제 사용자 질의는 단일 키워드에 대한 복합 주제(multi-section) 흐름을 포함하는 경우가 많음.\n",
    "    * 그러나, RAGAS 기본 데이터 생성 방식은 이를 추분히 반영하지 못함으로, 커스터마이징이 필수\n",
    "\n",
    "## 2. 변별력 강화를 위한 합성 데이터 설계\n",
    "* 좋은 평가 데이터는 다양한 RAG 시스템이나 LLM 모델 간의 성능 차이를 명확하게 드러낼 수 있어야 함\n",
    "* 이는 '변별력'을 지닌 질의로 정의\n",
    "* Multi-Hop 관련 시나리오 생성 신규 방안 도입을 통해 변별력을 가진 합성 데이터 생성  \n",
    "* 복합 유형(Simplt + Abstraction)을 통한 변별력 강화\n",
    "  * ex) \"역도 훈련 프로그램 구성에서 일반적인 훈련원칙은 무엇이고, 각 원칙에 대한 근거를 설명해줘.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS Customization 확인 절차\n",
    "1. ragas 기반의 기본 합성 데이터셋 생성\n",
    "2. 기본 합성 데이터셋 기반 검색 절차의 hyper parameter tuning 진행\n",
    "3. 검색 성능 평가의 분포를 고려하여 일부 hyper parameter 조합을 선정\n",
    "4. hyper parameter 조합을 대상으로 생성 절차의 hyper parameter tuning 진행\n",
    "5. custom 합성 데이터셋 생성\n",
    "6. custom 합성 데이터셋 기반 검색·생성 절차의 성능 평가\n",
    "7. 평가지표 비교를 통한 변별력 확인\n",
    "   * 성능 분산(분포)의 폭 비교\n",
    "   * 정렬 결과 차이 분석(Rank Sensitivity)\n",
    "   * 통계적 유의성 테스트\n",
    "\n",
    "변별력 확인이 어렵다면, 합성 데이터의 점수가 낮은지 높은지를 확인함.\n",
    "\n",
    "## hyper parameter 관련 주요 설정(auto_rag)\n",
    "1. 검색 평가 지표: [retrieval_f1, retrieval_ndcg, retrieval_map]\n",
    "2. bm25 tokenizer: ko_kiwi\n",
    "3. rrf_k(num_chunk): [3, 5, 10]\n",
    "4. 생성 평가 지표: bert_score 및 g_eval\n",
    "   * 여기서 ragas의 주요 지표를 사용해도 좋을거 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "with open('../data/document/역도/chunk_with_overlap.json', 'r', encoding='utf-8') as f:\n",
    "    origin_data = json.load(f)\n",
    "\n",
    "# custom dataset의 변별성 확인을 위해서 'Ⅲ. 역도경기 기술의 구조와 훈련법', 'Ⅳ. 역도체력의 구조와 훈련법'를 사용\n",
    "sample_data = origin_data[2] + origin_data[3]\n",
    "# sample_data = origin_data[2]\n",
    "print(len(sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 합성 데이터별 시나리오 출력\n",
    "2. MultihopAbstractQuery 최적화\n",
    "   * 병렬처리\n",
    "   * 노드별 이웃 노드 맵 생성\n",
    "3. 합성 데이터셋 번역 기능\n",
    "4. 합성 데이터셋 reference_contexts의 index_id 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ragas 기반 기본 합성 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model='gpt-4o-mini'))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "document_list = []\n",
    "\n",
    "for doc in sample_data:\n",
    "    page_content = doc['page_content']\n",
    "    metadata = doc['metadata']\n",
    "\n",
    "    new_document = Document(page_content)\n",
    "    new_document.metadata = metadata\n",
    "\n",
    "    document_list.append(new_document)\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={'page_content': doc['page_content'],\n",
    "                        'document_metadata': doc['metadata']}\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                               \r"
     ]
    }
   ],
   "source": [
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "trans = default_transforms(documents=document_list, llm=generator_llm, embedding_model=generator_embeddings)\n",
    "apply_transforms(kg, trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg.save('../data/document/역도/kg_sector3_4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = KnowledgeGraph.load('../data/document/역도/kg_sector3_4.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존 MultiHopAbstractQuerySunthesizer 개선 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "from ragas.testset.graph import KnowledgeGraph, Node\n",
    "from ragas.testset.synthesizers.multi_hop.abstract import MultiHopAbstractQuerySynthesizer\n",
    "from ragas.testset.synthesizers.single_hop.specific import SingleHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.multi_hop.specific import MultiHopSpecificQuerySynthesizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FastMultiHopAbstractQuerySynthesizer(MultiHopAbstractQuerySynthesizer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._neighbor_cache = {}\n",
    "        self._cluster_cache = {}\n",
    "        \n",
    "    def _build_neighbor_map(self, knowledge_graph: KnowledgeGraph) -> dict:\n",
    "        \"\"\"선처리: 노드별 이웃 노드 맵 생성\"\"\"\n",
    "        if not self._neighbor_cache:\n",
    "            neighbor_map = defaultdict(set)\n",
    "            # 한 번의 순회로 모든 관계 처리\n",
    "            for rel in knowledge_graph.relationships:\n",
    "                if rel.get_property(\"summary_similarity\"):\n",
    "                    neighbor_map[rel.source].add(rel.target)\n",
    "            self._neighbor_cache = dict(neighbor_map)\n",
    "        return self._neighbor_cache\n",
    "\n",
    "    def _find_cluster_from_node(self, start_node: Node, neighbor_map: dict, max_depth: int = 2) -> set:\n",
    "        \"\"\"단일 노드에서 시작하는 클러스터 찾기\"\"\"\n",
    "        # 캐시 확인\n",
    "        cache_key = (start_node.id, max_depth)\n",
    "        if cache_key in self._cluster_cache:\n",
    "            return self._cluster_cache[cache_key]\n",
    "\n",
    "        visited = {start_node}\n",
    "        current_level = {start_node}\n",
    "        \n",
    "        # BFS 사용 (더 효율적인 메모리 사용)\n",
    "        for depth in range(max_depth):\n",
    "            next_level = set()\n",
    "            for node in current_level:\n",
    "                neighbors = neighbor_map.get(node, set())\n",
    "                next_level.update(n for n in neighbors if n not in visited)\n",
    "            visited.update(next_level)\n",
    "            current_level = next_level\n",
    "            if not current_level:  # 더 이상 확장할 노드가 없으면 중단\n",
    "                break\n",
    "\n",
    "        # 결과 캐싱\n",
    "        self._cluster_cache[cache_key] = visited\n",
    "        return visited\n",
    "\n",
    "    def get_node_clusters(self, knowledge_graph: KnowledgeGraph) -> t.List[t.Set[Node]]:\n",
    "        \"\"\"최적화된 클러스터 찾기\"\"\"\n",
    "        # 1. 이웃 노드 맵 구축 (캐시 활용)\n",
    "        neighbor_map = self._build_neighbor_map(knowledge_graph)\n",
    "        \n",
    "        # 2. 병렬 처리를 위한 함수\n",
    "        def process_node_chunk(nodes):\n",
    "            return [self._find_cluster_from_node(node, neighbor_map) for node in nodes]\n",
    "\n",
    "        # 3. 노드를 청크로 분할하여 병렬 처리\n",
    "        chunk_size = max(1, len(knowledge_graph.nodes) // (4 * 2))  # CPU 코어 수의 2배 정도의 청크\n",
    "        node_chunks = [\n",
    "            list(knowledge_graph.nodes)[i:i + chunk_size]\n",
    "            for i in range(0, len(knowledge_graph.nodes), chunk_size)\n",
    "        ]\n",
    "\n",
    "        # 4. 병렬 처리 실행\n",
    "        all_clusters = []\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            chunk_results = list(executor.map(process_node_chunk, node_chunks))\n",
    "            for chunk_result in chunk_results:\n",
    "                all_clusters.extend(chunk_result)\n",
    "\n",
    "        # 5. 중복 제거 및 최소 크기 필터링 (set 연산 사용)\n",
    "        unique_clusters = set()\n",
    "        min_cluster_size = 2  # 최소 클러스터 크기 설정\n",
    "        \n",
    "        for cluster in all_clusters:\n",
    "            if len(cluster) >= min_cluster_size:\n",
    "                frozen_cluster = frozenset(cluster)\n",
    "                unique_clusters.add(frozen_cluster)\n",
    "\n",
    "        logger.info(f\"Found {len(unique_clusters)} unique clusters\")\n",
    "        return [set(cluster) for cluster in unique_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import typing as t\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "from ragas.testset.graph import KnowledgeGraph, Node\n",
    "\n",
    "from ragas.testset.synthesizers.multi_hop import MultiHopScenario\n",
    "from ragas.testset.synthesizers.multi_hop.abstract import MultiHopAbstractQuerySynthesizer\n",
    "from ragas.testset.synthesizers.single_hop.specific import SingleHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.multi_hop.specific import MultiHopSpecificQuerySynthesizer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class FastMultiHopAbstractQuerySynthesizer(MultiHopAbstractQuerySynthesizer):\n",
    "    name: str = \"fast_multi_hop_abstract_synthesizer\"\n",
    "    _scenario_cache: Dict = field(default_factory=dict)\n",
    "    generated_scenarios: List[MultiHopScenario] = field(default_factory=list)\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._neighbor_cache = {}\n",
    "        self._cluster_cache = {}\n",
    "        self.generated_scenarios = []\n",
    "        \n",
    "    def _build_neighbor_map(self, knowledge_graph: KnowledgeGraph) -> dict:\n",
    "        \"\"\"선처리: 노드별 이웃 노드 맵 생성\"\"\"\n",
    "        if not self._neighbor_cache:\n",
    "            neighbor_map = defaultdict(set)\n",
    "            # 한 번의 순회로 모든 관계 처리\n",
    "            for rel in knowledge_graph.relationships:\n",
    "                if rel.get_property(\"summary_similarity\"):\n",
    "                    neighbor_map[rel.source].add(rel.target)\n",
    "            self._neighbor_cache = dict(neighbor_map)\n",
    "        return self._neighbor_cache\n",
    "\n",
    "    def _find_cluster_from_node(self, start_node: Node, neighbor_map: dict, max_depth: int = 2) -> set:\n",
    "        \"\"\"단일 노드에서 시작하는 클러스터 찾기\"\"\"\n",
    "        # 캐시 확인\n",
    "        cache_key = (start_node.id, max_depth)\n",
    "        if cache_key in self._cluster_cache:\n",
    "            return self._cluster_cache[cache_key]\n",
    "\n",
    "        visited = {start_node}\n",
    "        current_level = {start_node}\n",
    "        \n",
    "        # BFS 사용 (더 효율적인 메모리 사용)\n",
    "        for depth in range(max_depth):\n",
    "            next_level = set()\n",
    "            for node in current_level:\n",
    "                neighbors = neighbor_map.get(node, set())\n",
    "                next_level.update(n for n in neighbors if n not in visited)\n",
    "            visited.update(next_level)\n",
    "            current_level = next_level\n",
    "            if not current_level:  # 더 이상 확장할 노드가 없으면 중단\n",
    "                break\n",
    "\n",
    "        # 결과 캐싱\n",
    "        self._cluster_cache[cache_key] = visited\n",
    "        return visited\n",
    "\n",
    "    def get_node_clusters(self, knowledge_graph: KnowledgeGraph) -> t.List[t.Set[Node]]:\n",
    "        \"\"\"최적화된 클러스터 찾기\"\"\"\n",
    "        # 1. 이웃 노드 맵 구축 (캐시 활용)\n",
    "        neighbor_map = self._build_neighbor_map(knowledge_graph)\n",
    "        \n",
    "        # 2. 병렬 처리를 위한 함수\n",
    "        def process_node_chunk(nodes):\n",
    "            return [self._find_cluster_from_node(node, neighbor_map) for node in nodes]\n",
    "\n",
    "        # 3. 노드를 청크로 분할하여 병렬 처리\n",
    "        chunk_size = max(1, len(knowledge_graph.nodes) // (4 * 2))  # CPU 코어 수의 2배 정도의 청크\n",
    "        node_chunks = [\n",
    "            list(knowledge_graph.nodes)[i:i + chunk_size]\n",
    "            for i in range(0, len(knowledge_graph.nodes), chunk_size)\n",
    "        ]\n",
    "\n",
    "        # 4. 병렬 처리 실행\n",
    "        all_clusters = []\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            chunk_results = list(executor.map(process_node_chunk, node_chunks))\n",
    "            for chunk_result in chunk_results:\n",
    "                all_clusters.extend(chunk_result)\n",
    "\n",
    "        # 5. 중복 제거 및 최소 크기 필터링 (set 연산 사용)\n",
    "        unique_clusters = set()\n",
    "        min_cluster_size = 2  # 최소 클러스터 크기 설정\n",
    "        \n",
    "        for cluster in all_clusters:\n",
    "            if len(cluster) >= min_cluster_size:\n",
    "                frozen_cluster = frozenset(cluster)\n",
    "                unique_clusters.add(frozen_cluster)\n",
    "\n",
    "        logger.info(f\"Found {len(unique_clusters)} unique clusters\")\n",
    "        return [set(cluster) for cluster in unique_clusters]\n",
    "\n",
    "    async def _generate_scenarios(\n",
    "        self,\n",
    "        n: int,\n",
    "        knowledge_graph: KnowledgeGraph,\n",
    "        persona_list: List,\n",
    "        callbacks,\n",
    "    ) -> List[MultiHopScenario]:\n",
    "\n",
    "        scenarios = await super()._generate_scenarios(n, knowledge_graph, persona_list, callbacks)\n",
    "\n",
    "        self.generated_scenarios.extend(scenarios)\n",
    "\n",
    "        return scenarios\n",
    "\n",
    "    def get_all_scenario_details(self):\n",
    "        details = []\n",
    "        for scenario in self.generated_scenarios:\n",
    "            detail = {\n",
    "                \"combinations\": scenario.combinations,\n",
    "                \"persona\": {\n",
    "                    \"name\": scenario.persona.name,\n",
    "                    \"description\": scenario.persona.role_description\n",
    "                },\n",
    "                \"query_style\": scenario.style.name,\n",
    "                \"query_length\": scenario.length.name\n",
    "            }\n",
    "            \n",
    "            details.append(detail)\n",
    "        return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ragas testset generation chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new persona_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new persona_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new persona_generation_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas:  33%|███▎      | 1/3 [00:00<00:01,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas: 100%|██████████| 3/3 [00:01<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new Scenario Generation chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_specific_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_abstract_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new concept_combination_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new concept_combination_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new concept_combination_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Scenarios:  33%|███▎      | 1/3 [00:07<00:15,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new concept_combination_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new concept_combination_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new concept_combination_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new concept_combination_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new concept_combination_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Scenarios:  67%|██████▋   | 2/3 [00:19<00:09,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new themes_personas_matching_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Scenarios: 100%|██████████| 3/3 [00:29<00:00,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new Sample Generation chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:   0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new single_hop_specifc_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_specific_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:   3%|▎         | 1/31 [00:01<00:58,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_specific_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:   6%|▋         | 2/31 [00:02<00:34,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_specific_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  13%|█▎        | 4/31 [00:03<00:17,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_specific_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_specific_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_specific_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  26%|██▌       | 8/31 [00:03<00:05,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_specific_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_specific_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_abstract_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_abstract_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  39%|███▊      | 12/31 [00:04<00:02,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_abstract_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_abstract_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_abstract_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_abstract_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  45%|████▌     | 14/31 [00:04<00:02,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_abstract_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  48%|████▊     | 15/31 [00:04<00:02,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new multi_hop_abstract_query_synthesizer chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new query_answer_generation_prompt chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  52%|█████▏    | 16/31 [00:05<00:03,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  58%|█████▊    | 18/31 [00:05<00:02,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  61%|██████▏   | 19/31 [00:05<00:02,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  74%|███████▍  | 23/31 [00:06<00:01,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  84%|████████▍ | 26/31 [00:06<00:00,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  87%|████████▋ | 27/31 [00:07<00:01,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  90%|█████████ | 28/31 [00:07<00:00,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples:  97%|█████████▋| 30/31 [00:08<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples: 100%|██████████| 31/31 [00:10<00:00,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings, knowledge_graph=kg)\n",
    "\n",
    "query_distribution = [\n",
    "    (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
    "    (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
    "    (FastMultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25)\n",
    "]\n",
    " \n",
    "from langchain_core.callbacks.stdout import StdOutCallbackHandler\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "# testset = generator.generate(testset_size=30, \n",
    "#                              query_distribution=query_distribution,\n",
    "#                              callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df = testset.to_pandas()\n",
    "# testset_df.to_csv('../data/document/역도/df_sector3_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an expert translator specializing in English to Korean translation.\n",
    "\n",
    "    Translate the following English text into natural Korean.  \n",
    "    Only output the translated Korean text.  \n",
    "    If a term is a proper noun or a commonly used English term (e.g., \"clean and jerk\"), transliterate it into Korean and include the original English in parentheses.\n",
    "\n",
    "    Text:  \n",
    "    {input_text}\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_language(text):\n",
    "    english_count = len(re.findall(r'[a-zA-Z]', text))\n",
    "    korean_count = len(re.findall(r'[가-힣]', text))\n",
    "\n",
    "    if english_count >= korean_count:\n",
    "        return 'english'\n",
    "\n",
    "    return 'korean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df['language_user_input'] = testset_df['user_input'].apply(lambda x : classify_language(x))\n",
    "testset_df['language_reference'] = testset_df['reference'].apply(lambda x : classify_language(x))\n",
    "\n",
    "user_data = testset_df.loc[(testset_df['language_user_input'] == 'english'), 'user_input'].tolist()\n",
    "reference_data = testset_df.loc[(testset_df['language_reference'] == 'english'), 'reference'].tolist()\n",
    "\n",
    "translate_user = chain.batch(user_data, config={'max_concurrency': 5})\n",
    "translate_reference = chain.batch(reference_data, config={'max_concurrency': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df.loc[(testset_df['language_user_input'] == 'english'), 'user_input'] = translate_user\n",
    "testset_df.loc[(testset_df['language_reference'] == 'english'), 'reference'] = translate_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df.iloc[:, :4].to_csv('../data/document/역도/df_sector3_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>language_user_input</th>\n",
       "      <th>language_reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>역도에서 스내치(snatch) 기술의 기본 원리는 무엇인가요?</td>\n",
       "      <td>[역도경기의 기술이라 함은, 경기자가 극한의 중량을 가진 바벨을 들어올리기 위\\n하...</td>\n",
       "      <td>역도에서 스내치(snatch) 기술의 기본 원리는 최소한의 노력으로 최대 중량을 들...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>english</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>오버그립의 정의는 무엇인가요?</td>\n",
       "      <td>[바벨을 잡는 방법에는 크게 오버그립(over grip), 언더그립(under gr...</td>\n",
       "      <td>오버그립(over grip)은 손바닥을 몸 쪽으로 향하여 위에서 바벨을 잡는 방법입니다.</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>korean</td>\n",
       "      <td>korean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           user_input  \\\n",
       "0  역도에서 스내치(snatch) 기술의 기본 원리는 무엇인가요?   \n",
       "1                    오버그립의 정의는 무엇인가요?   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [역도경기의 기술이라 함은, 경기자가 극한의 중량을 가진 바벨을 들어올리기 위\\n하...   \n",
       "1  [바벨을 잡는 방법에는 크게 오버그립(over grip), 언더그립(under gr...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  역도에서 스내치(snatch) 기술의 기본 원리는 최소한의 노력으로 최대 중량을 들...   \n",
       "1  오버그립(over grip)은 손바닥을 몸 쪽으로 향하여 위에서 바벨을 잡는 방법입니다.   \n",
       "\n",
       "                       synthesizer_name language_user_input language_reference  \n",
       "0  single_hop_specifc_query_synthesizer             english            english  \n",
       "1  single_hop_specifc_query_synthesizer              korean             korean  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS 합성 데이터셋 생성 관련 시나리오 출력\n",
    "* 개선점\n",
    "  * 한번 생성된 지식그래프를 저장하는 코드 필요\n",
    "  * 각종 단계를 출력하는 코드\n",
    "  * 페르소나에서 role_description도 출력하도록 변경 필요\n",
    "  * 시나리오 자체도 출력하도록 수정 필요\n",
    "  * 나중에는 지식그래프 입력하고 transform만 입력하면, 문제만 생성 하도록 (?)\n",
    "  * summary를 실행하지 않도록 하는 방법(?)\n",
    "  * 영어로 출력되는 문제 해결하기\n",
    "* 지금 해야하는 것\n",
    "  * 작성된 코드를 분석하기\n",
    "  * 분석 결과를 기준으로 개선하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from ragas.testset.synthesizers.single_hop import SingleHopScenario, SingleHopQuerySynthesizer\n",
    "from ragas.testset.synthesizers.multi_hop import MultiHopScenario, MultiHopQuerySynthesizer\n",
    "\n",
    "@dataclass\n",
    "class CustomSingleHopSpecificSynthesizer(SingleHopSpecificQuerySynthesizer):\n",
    "    name: str = \"custom_single_hop_specific_synthesizer\"\n",
    "    _scenario_cache: Dict = field(default_factory=dict)\n",
    "    generated_scenarios: List[SingleHopScenario] = field(default_factory=list, init=False)\n",
    "\n",
    "    async def _generate_scenarios(\n",
    "        self,\n",
    "        n: int,\n",
    "        knowledge_graph: KnowledgeGraph,\n",
    "        persona_list: List,\n",
    "        callbacks,\n",
    "    ) -> List[SingleHopScenario]:\n",
    "        # 부모 클래스의 _generate_scenarios 메서드 호출\n",
    "        scenarios = await super()._generate_scenarios(n, knowledge_graph, persona_list, callbacks)\n",
    "        \n",
    "        # 생성된 시나리오들을 저장\n",
    "        self.generated_scenarios.extend(scenarios)\n",
    "        \n",
    "        return scenarios\n",
    "    \n",
    "    def get_all_scenario_details(self):\n",
    "        details = []\n",
    "        for scenario in self.generated_scenarios:\n",
    "            detail = {\n",
    "                \"term\": scenario.term,\n",
    "                \"persona\": {\n",
    "                    \"name\": scenario.persona.name,\n",
    "                    \"description\": scenario.persona.role_description\n",
    "                },\n",
    "                \"query_style\": scenario.style.name,\n",
    "                \"query_length\": scenario.length.name\n",
    "            }\n",
    "            \n",
    "            details.append(detail)\n",
    "        return details\n",
    "\n",
    "@dataclass\n",
    "class CustomMultiHopSpecificSynthesizer(MultiHopSpecificQuerySynthesizer):\n",
    "    name: str = \"custom_multi_hop_specific_synthesizer\"\n",
    "    _scenario_cache: Dict = field(default_factory=dict)\n",
    "    generated_scenarios: List[MultiHopScenario] = field(default_factory=list, init=False)\n",
    "\n",
    "    async def _generate_scenarios(\n",
    "        self,\n",
    "        n: int,\n",
    "        knowledge_graph: KnowledgeGraph,\n",
    "        persona_list: List,\n",
    "        callbacks,\n",
    "    ) -> List[MultiHopScenario]:\n",
    "        # 부모 클래스의 _generate_scenarios 메서드 호출\n",
    "        scenarios = await super()._generate_scenarios(n, knowledge_graph, persona_list, callbacks)\n",
    "        \n",
    "        # 생성된 시나리오들을 저장\n",
    "        self.generated_scenarios.extend(scenarios)\n",
    "        \n",
    "        return scenarios\n",
    "\n",
    "    def get_all_scenario_details(self):\n",
    "        details = []\n",
    "        for scenario in self.generated_scenarios:\n",
    "            detail = {\n",
    "                \"combinations\": scenario.combinations,\n",
    "                \"persona\": {\n",
    "                    \"name\": scenario.persona.name,\n",
    "                    \"description\": scenario.persona.role_description\n",
    "                },\n",
    "                \"query_style\": scenario.style.name,\n",
    "                \"query_length\": scenario.length.name\n",
    "            }\n",
    "            \n",
    "            details.append(detail)\n",
    "        return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any, Tuple, Union\n",
    "from ragas.testset.synthesizers.generate import TestsetGenerator\n",
    "from ragas.testset.synthesizers.testset_schema import Testset, TestsetSample\n",
    "from ragas.testset.synthesizers.base import BaseScenario\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from ragas.testset.persona import generate_personas_from_kg\n",
    "from ragas.testset.synthesizers.utils import calculate_split_values\n",
    "from ragas.executor import Executor\n",
    "\n",
    "\n",
    "class CustomTestGenerator(TestsetGenerator):\n",
    "    \"\"\"\n",
    "    TestGenerator를 상속받아 각 데이터 행별 시나리오 정보를 포함하는 커스텀 생성기\n",
    "    \"\"\"\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        testset_size: int,\n",
    "        query_distribution: Optional[List[tuple]] = None,\n",
    "        num_personas: int = 3,\n",
    "        run_config: Optional[Dict[str, Any]] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        callbacks: Optional[List] = None,\n",
    "        token_usage_parser: Optional[Any] = None,\n",
    "        with_debugging_logs: bool = False,\n",
    "        raise_exceptions: bool = True,\n",
    "    ) -> Testset:\n",
    "        \"\"\"\n",
    "        기존 generate 메소드를 오버라이드하여 시나리오 정보를 포함하도록 수정\n",
    "        \"\"\"\n",
    "        if run_config is not None:\n",
    "            self.llm.set_run_config(run_config)\n",
    "\n",
    "        query_distribution = query_distribution or default_query_distribution(\n",
    "            self.llm, self.knowledge_graph\n",
    "        )\n",
    "        callbacks = callbacks or []\n",
    "\n",
    "        # 페르소나 생성\n",
    "        if self.persona_list is None:\n",
    "            self.persona_list = generate_personas_from_kg(\n",
    "                llm=self.llm,\n",
    "                kg=self.knowledge_graph,\n",
    "                num_personas=num_personas,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "        else:\n",
    "            random.shuffle(self.persona_list)\n",
    "\n",
    "        # 시나리오 생성\n",
    "        splits, _ = calculate_split_values(\n",
    "            [prob for _, prob in query_distribution], testset_size\n",
    "        )\n",
    "        exec = Executor(\n",
    "            desc=\"Generating Scenarios\",\n",
    "            raise_exceptions=raise_exceptions,\n",
    "            run_config=run_config,\n",
    "            keep_progress_bar=False,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        \n",
    "        for i, (scenario, _) in enumerate(query_distribution):\n",
    "            exec.submit(\n",
    "                scenario.generate_scenarios,\n",
    "                n=splits[i],\n",
    "                knowledge_graph=self.knowledge_graph,\n",
    "                persona_list=self.persona_list[:num_personas],\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "\n",
    "        scenario_sample_list: t.List[t.List[BaseScenario]] = exec.results()\n",
    "\n",
    "        # 샘플 생성\n",
    "        exec = Executor(\n",
    "            \"Generating Samples\",\n",
    "            raise_exceptions=raise_exceptions,\n",
    "            run_config=run_config,\n",
    "            keep_progress_bar=True,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        \n",
    "        additional_testset_info: t.List[t.Dict] = []\n",
    "        for i, (synthesizer, _) in enumerate(query_distribution):\n",
    "            for scenario in scenario_sample_list[i]:\n",
    "                exec.submit(\n",
    "                    synthesizer.generate_sample,\n",
    "                    scenario=scenario,\n",
    "                    callbacks=callbacks,\n",
    "                )\n",
    "                # 시나리오 정보를 additional_info에 추가\n",
    "                additional_testset_info.append({\n",
    "                    \"synthesizer_name\": synthesizer.name,\n",
    "                    \"scenario_info\": {\n",
    "                        \"type\": scenario.__class__.__name__,\n",
    "                        \"description\": str(scenario),\n",
    "                        \"style\": str(scenario.style),\n",
    "                        \"length\": str(scenario.length),\n",
    "                        \"nodes\": [str(node) for node in scenario.nodes]\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        eval_samples = exec.results()\n",
    "\n",
    "        # 테스트셋 생성\n",
    "        testsets = []\n",
    "        for sample, additional_info in zip(eval_samples, additional_testset_info):\n",
    "            testsets.append(TestsetSample(eval_sample=sample, **additional_info))\n",
    "            \n",
    "        testset = Testset(samples=testsets)\n",
    "        return testset \n",
    "\n",
    "    def _generate_batch(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        query_distribution: List[Tuple[Union[SingleHopQuerySynthesizer, MultiHopQuerySynthesizer], float]],\n",
    "        callbacks: List[StdOutCallbackHandler] = None\n",
    "    ) -> Tuple[Any, Dict[str, List[Dict[str, Any]]]]:\n",
    "        \n",
    "        # 데이터셋 생성\n",
    "        dataset = self.generate(\n",
    "            testset_size=batch_size,\n",
    "            query_distribution=query_distribution,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # 시나리오 상세 정보 수집\n",
    "        scenario_details = {\n",
    "            \"single_hop\": [],\n",
    "            \"multi_hop\": [],\n",
    "            \"fast_multi_hop\": []\n",
    "        }\n",
    "        \n",
    "        # 각 Synthesizer의 시나리오 정보 수집\n",
    "        for synthesizer, _ in query_distribution:\n",
    "            if isinstance(synthesizer, CustomSingleHopSpecificSynthesizer):\n",
    "                details = synthesizer.get_all_scenario_details()\n",
    "                scenario_details[\"single_hop\"].extend(details)\n",
    "                synthesizer.generated_scenarios = []  # 다음 배치를 위해 초기화\n",
    "                \n",
    "            elif isinstance(synthesizer, CustomMultiHopSpecificSynthesizer):\n",
    "                details = synthesizer.get_all_scenario_details()\n",
    "                scenario_details[\"multi_hop\"].extend(details)\n",
    "                synthesizer.generated_scenarios = []  # 다음 배치를 위해 초기화\n",
    "            \n",
    "            elif isinstance(synthesizer, FastMultiHopAbstractQuerySynthesizer):\n",
    "                details = synthesizer.get_all_scenario_details()\n",
    "                scenario_details['fast_multi_hop'].extend(details)\n",
    "                synthesizer.generated_scenarios = []\n",
    "        \n",
    "        return dataset, scenario_details\n",
    "\n",
    "    def merge_scenario(\n",
    "        self,\n",
    "        final_dataset: pd.DataFrame,\n",
    "        all_scenario_details: Dict[str, List[Dict[str, Any]]],\n",
    "    ) -> pd.DataFrame:\n",
    "        scenario_info_list = []\n",
    "        for scenario_type, details in all_scenario_details.items():\n",
    "            for detail in details:\n",
    "                scenario_info = {\n",
    "                    'scenario_type': scenario_type,\n",
    "                    'combinations': detail.get('combinations', ''),\n",
    "                    'term': detail.get('term', ''),\n",
    "                    'persona_name': detail.get('persona', {}).get('name', ''),\n",
    "                    'persona_description': detail.get('persona', {}).get('description', ''),\n",
    "                    'query_style': detail.get('query_style', ''),\n",
    "                    'query_length': detail.get('query_length', '')\n",
    "                }\n",
    "                scenario_info_list.append(scenario_info)\n",
    "        \n",
    "        scenario_df = pd.DataFrame(scenario_info_list)\n",
    "        \n",
    "        for col in scenario_df.columns:\n",
    "            final_dataset[col] = scenario_df[col].values\n",
    "        return final_dataset\n",
    "        \n",
    "    def generate_with_details(\n",
    "        self,\n",
    "        testset_size: int,\n",
    "        query_distribution: List[Tuple[Union[SingleHopQuerySynthesizer, MultiHopQuerySynthesizer], float]],\n",
    "        callbacks: List[StdOutCallbackHandler] = None,\n",
    "        batch_size: int = 5\n",
    "    ) -> Tuple[Any, Dict[str, List[Dict[str, Any]]]]:\n",
    "        \"\"\"배치 처리와 병렬 처리를 통한 데이터셋 생성\"\"\"\n",
    "        all_datasets = []\n",
    "        all_scenario_details = {\n",
    "            \"single_hop\": [],\n",
    "            \"multi_hop\": [],\n",
    "            \"fast_multi_hop\": []\n",
    "        }\n",
    "        \n",
    "        with tqdm(total=testset_size, desc=\"데이터셋 생성 중\") as pbar:\n",
    "            for i in range(0, testset_size, batch_size):\n",
    "                current_batch_size = min(batch_size, testset_size - i)\n",
    "                \n",
    "                dataset, scenario_details = self._generate_batch(\n",
    "                    batch_size=current_batch_size,\n",
    "                    query_distribution=query_distribution,\n",
    "                    # callbacks=callbacks\n",
    "                )\n",
    "\n",
    "                all_datasets.append(dataset)\n",
    "                for key in all_scenario_details:\n",
    "                    all_scenario_details[key].extend(scenario_details[key])\n",
    "                \n",
    "                pbar.update(current_batch_size)\n",
    "        \n",
    "        # 데이터셋 병합\n",
    "        final_dataset = pd.concat([d.to_pandas() for d in all_datasets], ignore_index=True)\n",
    "\n",
    "        merged_dataset = self.merge_scenario(final_dataset, all_scenario_details)\n",
    "        \n",
    "        return merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm import tqdm\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "def run_generation(generator_llm, generator_embeddings, kg):\n",
    "    # CustomTestsetGenerator 인스턴스 생성\n",
    "    generator = CustomTestGenerator(\n",
    "        llm=generator_llm,\n",
    "        embedding_model=generator_embeddings,\n",
    "        knowledge_graph=kg,\n",
    "        # output_dir=\"my_dataset\" # 필요에 따라 출력 디렉토리 지정\n",
    "    )\n",
    "\n",
    "    # Synthesizer 설정\n",
    "    query_distribution = [\n",
    "        (CustomSingleHopSpecificSynthesizer(llm=generator_llm), 0.5),\n",
    "        (CustomMultiHopSpecificSynthesizer(llm=generator_llm), 0.25),\n",
    "        (FastMultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25)\n",
    "\n",
    "    ]\n",
    "    \n",
    "    # 콜백 설정 (선택 사항)\n",
    "    callbacks = [StdOutCallbackHandler()]\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    merged_dataset = generator.generate_with_details(\n",
    "        testset_size=10,\n",
    "        query_distribution=query_distribution,\n",
    "        batch_size=5\n",
    "        # callbacks=callbacks, # 콜백 전달\n",
    "    )\n",
    "\n",
    "    return merged_dataset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "데이터셋 생성 중:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s]\n",
      "Generating Scenarios: 100%|██████████| 3/3 [00:06<00:00,  2.10s/it]\n",
      "Generating Samples: 100%|██████████| 7/7 [00:07<00:00,  1.10s/it]\n",
      "Generating Scenarios: 100%|██████████| 3/3 [00:08<00:00,  2.73s/it]\n",
      "Generating Samples: 100%|██████████| 7/7 [00:05<00:00,  1.17it/s]\n",
      "데이터셋 생성 중: 100%|██████████| 10/10 [00:29<00:00,  2.98s/it]\n"
     ]
    }
   ],
   "source": [
    "merged_dataset = run_generation(generator_llm, generator_embeddings, kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>scenario_type</th>\n",
       "      <th>combinations</th>\n",
       "      <th>term</th>\n",
       "      <th>persona_name</th>\n",
       "      <th>persona_description</th>\n",
       "      <th>query_style</th>\n",
       "      <th>query_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>리프팅 기술이란 뭐고, 어떻게 선수의 성능에 영향을 미치나?</td>\n",
       "      <td>[역도경기의 기술이라 함은, 경기자가 극한의 중량을 가진 바벨을 들어올리기 위\\n하...</td>\n",
       "      <td>리프팅 기술은 경기자가 극한의 중량을 가진 바벨을 들어올리기 위해 육체적 성능을 합...</td>\n",
       "      <td>custom_single_hop_specific_synthesizer</td>\n",
       "      <td>single_hop</td>\n",
       "      <td></td>\n",
       "      <td>리프팅</td>\n",
       "      <td>Sports Performance Analyst</td>\n",
       "      <td>Analyzes athletic performance and biomechanica...</td>\n",
       "      <td>POOR_GRAMMAR</td>\n",
       "      <td>MEDIUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you explain the significance of the 훅그립 in...</td>\n",
       "      <td>[바벨을 잡는 방법에는 크게 오버그립(over grip), 언더그립(under gr...</td>\n",
       "      <td>훅그립(hook grip) is a grip technique used by all...</td>\n",
       "      <td>custom_single_hop_specific_synthesizer</td>\n",
       "      <td>single_hop</td>\n",
       "      <td></td>\n",
       "      <td>훅그립</td>\n",
       "      <td>Sports Performance Analyst</td>\n",
       "      <td>Analyzes athletic performance and biomechanica...</td>\n",
       "      <td>PERFECT_GRAMMAR</td>\n",
       "      <td>LONG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                  리프팅 기술이란 뭐고, 어떻게 선수의 성능에 영향을 미치나?   \n",
       "1  Can you explain the significance of the 훅그립 in...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [역도경기의 기술이라 함은, 경기자가 극한의 중량을 가진 바벨을 들어올리기 위\\n하...   \n",
       "1  [바벨을 잡는 방법에는 크게 오버그립(over grip), 언더그립(under gr...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  리프팅 기술은 경기자가 극한의 중량을 가진 바벨을 들어올리기 위해 육체적 성능을 합...   \n",
       "1  훅그립(hook grip) is a grip technique used by all...   \n",
       "\n",
       "                         synthesizer_name scenario_type combinations term  \\\n",
       "0  custom_single_hop_specific_synthesizer    single_hop               리프팅   \n",
       "1  custom_single_hop_specific_synthesizer    single_hop               훅그립   \n",
       "\n",
       "                 persona_name  \\\n",
       "0  Sports Performance Analyst   \n",
       "1  Sports Performance Analyst   \n",
       "\n",
       "                                 persona_description      query_style  \\\n",
       "0  Analyzes athletic performance and biomechanica...     POOR_GRAMMAR   \n",
       "1  Analyzes athletic performance and biomechanica...  PERFECT_GRAMMAR   \n",
       "\n",
       "  query_length  \n",
       "0       MEDIUM  \n",
       "1         LONG  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def make_chunk_dict(knowledge_graph):\n",
    "    chunk_dict = {}\n",
    "    for node in kg.nodes:\n",
    "        chunk_id = node.properties['document_metadata']['chunk_id']\n",
    "        page_content = node.properties['page_content']\n",
    "\n",
    "        chunk_dict[page_content] = chunk_id\n",
    "    return chunk_dict\n",
    "\n",
    "def regular_expression(reference_contexts, chunk_dict):\n",
    "    if len(reference_contexts) == 1:\n",
    "        return [chunk_dict[reference_contexts[0]]]\n",
    "    else:\n",
    "        return [chunk_dict[re.sub(r\"<\\d+-hop>\\n\\n\", \"\", text)] for text in reference_contexts] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reference_contexts(knowledge_graph, merged_dataset):\n",
    "    chunk_dict = make_chunk_dict(knowledge_graph)\n",
    "    merged_dataset['reference_contexts_id'] = merged_dataset['reference_contexts'].apply(lambda x : regular_expression(x, chunk_dict))\n",
    "\n",
    "    return merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = make_reference_contexts(kg, merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS 실험\n",
    "### 평가 항목\n",
    "1. Generation \n",
    "   1. Faithfulness\n",
    "      * 주어진 문맥에 대한 생성된 답변의 사실적 일관성 측정\n",
    "      * 답변과 검색된 문맥(retrieved context)를 기준으로 계산\n",
    "      * (0, 1) 범위 스케일이며, 값이 높을수록 좋음\n",
    "      * 생성된 답변이 신뢰할 수 있다고(faithful) 간구되려면 답변에서 제시된 모든 주장이 주어진 문맥(given context)에서 추론될 수 있어야 함\n",
    "      * 생성된 답변에서 주장의 집합(claims)를 식별 -> 각 주장마다 주어진 맥락 기반 여부 확인\n",
    "      * 점수: context 기반의 답변 내 주장 수 / 전체 주장 수\n",
    "      * 예시\n",
    "        * 아이슈타인의 출생일자와 출생지는 어디인가?\n",
    "          * 답변 1: 아이슈타인은 독엘에서 1879/3/14에 태어났습니다.\n",
    "            * \n",
    "          * 답변 2: 아이슈타인은 독엘에서 1879/4/14에 태어났습니다.\n",
    "            * \n",
    "          * context: \n",
    "* \n",
    "   1. \n",
    "1. Retriever\n",
    "   1. \n",
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SportAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
