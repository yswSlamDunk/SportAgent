{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_open(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/document/역도\\역도의 이해_dataset.json\n",
      "../data/document/역도\\역도의 스포츠 과학적 원리_dataset.json\n",
      "../data/document/역도\\역도경기 기술의 구조와 훈련법_dataset.json\n",
      "../data/document/역도\\역도체력의 구조와 훈련법_dataset.json\n",
      "../data/document/역도\\역도 훈련프로그램 구성 및 지도안_dataset.json\n"
     ]
    }
   ],
   "source": [
    "base_path = '../data/document/역도'\n",
    "\n",
    "name_list = ['역도의 이해', '역도의 스포츠 과학적 원리', '역도경기 기술의 구조와 훈련법', '역도체력의 구조와 훈련법', '역도 훈련프로그램 구성 및 지도안']\n",
    "path_list = []\n",
    "\n",
    "suffix = '_dataset.json'\n",
    "for name in name_list:\n",
    "    path = os.path.join(base_path, name + suffix)\n",
    "    path_list.append(path)\n",
    "\n",
    "dataset_list = []\n",
    "for path in path_list:\n",
    "    print(path)\n",
    "    file = file_open(path)\n",
    "    dataset_list.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 전처리\n",
    "## 검색 전용 데이터 생성\n",
    "1. 불필요 정보 제거\n",
    "   - figure, chart, table의 hypotheticalQuestions 제거(?)\n",
    "   - filepaths 제거\n",
    "   - category 제거 \n",
    "2. 페이지 수정\n",
    "3. tiktoken 수를 기준으로 분할\n",
    "   - 최소 토큰 350, 최대 토큰 500 \n",
    "   - overlap: 최대 150 토큰\n",
    "\n",
    "### 검색 전용 Document 구조\n",
    "- id: int\n",
    "- metadata: Dict\n",
    "  - filename: str\n",
    "  - page: List[int]\n",
    "- page_content: str\n",
    "- type: Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def get_token_length(text: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def split_into_sentences(text: str) -> List[Tuple[str, int]]:\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    return [(s.strip(), get_token_length(s.strip())) for s in sentences if s.strip()]\n",
    "\n",
    "def get_overlap_from_previous_chunk(\n",
    "    last_chunk: dict,\n",
    "    current_chunk_tokens: int,\n",
    "    current_chunk_sentences: List[Tuple[str, int]],\n",
    "    min_tokens: int,\n",
    "    max_tokens: int\n",
    "    ) -> Tuple[List[Tuple[str, int]], int]:\n",
    "\n",
    "    last_sentences = split_into_sentences(last_chunk['page_content'])\n",
    "    \n",
    "    # 필요한 토큰 수와 가용 가능한 토큰 수 계산\n",
    "    needed_tokens = min_tokens - current_chunk_tokens\n",
    "    available_tokens = max_tokens - current_chunk_tokens\n",
    "    \n",
    "    additional_sentences = []\n",
    "    additional_tokens = 0\n",
    "    \n",
    "    # 이전 청크의 문장들을 뒤에서부터 검사\n",
    "    for sent, tok in reversed(last_sentences):\n",
    "        # 현재 청크가 min_tokens보다 작은 경우\n",
    "        if current_chunk_tokens < min_tokens:\n",
    "            if additional_tokens + tok <= available_tokens:\n",
    "                additional_sentences.insert(0, (sent, tok))\n",
    "                additional_tokens += tok\n",
    "                if additional_tokens >= needed_tokens:\n",
    "                    break\n",
    "        # 일반적인 경우\n",
    "        else:\n",
    "            if (current_chunk_tokens + additional_tokens + tok <= max_tokens):\n",
    "                additional_sentences.insert(0, (sent, tok))\n",
    "                additional_tokens += tok\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # 확장된 문장 리스트와 토큰 수 생성\n",
    "    extended_sentences = additional_sentences + current_chunk_sentences\n",
    "    extended_tokens = current_chunk_tokens + additional_tokens\n",
    "    \n",
    "    return extended_sentences, extended_tokens\n",
    "\n",
    "\n",
    "def chunk_by_token_range_with_sentence_overlap(\n",
    "    doc: dict,\n",
    "    chunk_id: int,\n",
    "    min_tokens: int = 350,\n",
    "    max_tokens: int = 500,\n",
    "    ) -> Tuple[List[dict], List[dict], int]:    \n",
    "\n",
    "    json_list = []\n",
    "    page_content = ''\n",
    "    for element in doc['content']:\n",
    "        if element['category'] == 'table':\n",
    "            caption = '' \n",
    "            if 'caption' in element:\n",
    "                caption = '\\n\\n' + element['caption'] + '\\n\\n' # llm에 context를 입력하는 과정에서 caption 활용을 위한 전처리\n",
    "            json = element['information']['table_json']\n",
    "            chunk_id += 1\n",
    "            json_list.append({'chunk_id': chunk_id,\n",
    "                              'table': json,\n",
    "                              'caption': caption})\n",
    "\n",
    "        if element['category'] in ['figure', 'chart', 'table']:\n",
    "            if 'caption' in element['information']:\n",
    "                caption = element['information']['caption']\n",
    "                page_content += caption + '\\n'\n",
    "\n",
    "            detail = element['information']['detail']\n",
    "            page_content += detail + '\\n'\n",
    "        \n",
    "        if element['category'] == 'paragraph':\n",
    "            page_content += element['information'] + '\\n'\n",
    "\n",
    "    sentences = split_into_sentences(page_content)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_tokens = 0\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        sentence, token_count = sentences[i]\n",
    "        # 한 문장이 max_tokens보다 큰 경우\n",
    "        if token_count > max_tokens:\n",
    "            if current_chunk_sentences:\n",
    "                if chunks:\n",
    "                    # overlap 처리를 별도 함수로 분리\n",
    "                    extended_sentences, extended_tokens = get_overlap_from_previous_chunk(\n",
    "                        last_chunk=chunks[-1],\n",
    "                        current_chunk_tokens=current_chunk_tokens,\n",
    "                        current_chunk_sentences=current_chunk_sentences,\n",
    "                        min_tokens=min_tokens,\n",
    "                        max_tokens=max_tokens\n",
    "                    )\n",
    "                    \n",
    "                    chunk_id += 1\n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'page_content': ' '.join([s for s, _ in extended_sentences]),\n",
    "                    })\n",
    "                else:\n",
    "                    # 첫 번째 청크인 경우\n",
    "                    chunk_id += 1\n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'page_content': ' '.join([s for s, _ in current_chunk_sentences]),\n",
    "                    })\n",
    "            \n",
    "            # 긴 문장을 별도 청크로 저장\n",
    "            chunk_id += 1\n",
    "            chunks.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'page_content': sentence,\n",
    "            })\n",
    "            \n",
    "            current_chunk_sentences = []\n",
    "            current_chunk_tokens = 0\n",
    "            i += 1\n",
    "        \n",
    "        # 일반적인 문장 처리\n",
    "        else:            \n",
    "            # 현재 청크가 min_tokens를 넘었고, 새 문장 추가시 max_tokens를 초과하는 경우\n",
    "            if (current_chunk_tokens >= min_tokens) and (current_chunk_tokens < max_tokens):\n",
    "                if chunks:\n",
    "                    # overlap 처리\n",
    "                    extended_sentences, extended_tokens = get_overlap_from_previous_chunk(\n",
    "                        last_chunk=chunks[-1],\n",
    "                        current_chunk_tokens=current_chunk_tokens,\n",
    "                        current_chunk_sentences=current_chunk_sentences,\n",
    "                        min_tokens=min_tokens,\n",
    "                        max_tokens=max_tokens\n",
    "                    )\n",
    "                    \n",
    "                    chunk_id += 1\n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'page_content': ' '.join([s for s, _ in extended_sentences]),\n",
    "                    })\n",
    "                else:\n",
    "                    # 첫 번째 청크인 경우\n",
    "                    chunk_id += 1\n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'page_content': ' '.join([s for s, _ in current_chunk_sentences]),\n",
    "                    })\n",
    "                \n",
    "                # 새로운 청크 시작\n",
    "                current_chunk_sentences = [(sentence, token_count)]\n",
    "                current_chunk_tokens = token_count\n",
    "            \n",
    "            else:\n",
    "                current_chunk_sentences.append((sentence, token_count))\n",
    "                current_chunk_tokens = current_chunk_tokens + token_count\n",
    "            \n",
    "            i += 1\n",
    "    return chunks, json_list, chunk_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap_from_previous_chunk(\n",
    "    last_chunk: dict,\n",
    "    current_chunk_tokens: int,\n",
    "    current_chunk_sentences: List[Tuple[str, int]],\n",
    "    min_tokens: int,\n",
    "    max_tokens: int,\n",
    "    ) -> Tuple[List[Tuple[str, int]], int]:\n",
    "    last_sentences = split_into_sentences(last_chunk['page_content'])\n",
    "\n",
    "    max_addable_tokens = max(0, max_tokens - current_chunk_tokens)\n",
    "\n",
    "    additional_sentences = []\n",
    "    additional_tokens = 0\n",
    "\n",
    "    for sent, tok in reversed(last_sentences):\n",
    "        if additional_tokens + tok <= max_addable_tokens:\n",
    "            additional_sentences.insert(0, (sent, tok))\n",
    "            additional_tokens += tok\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    extended_sentences = additional_sentences + current_chunk_sentences\n",
    "    extended_tokens = current_chunk_tokens + additional_tokens\n",
    "    \n",
    "    return extended_sentences, extended_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def make_dataset_for_search(dataset, before_page, before_id, chunk_id, chunk_size_min=350, chunk_size_max=500):\n",
    "    new_dataset = []\n",
    "    new_table_dataset = []\n",
    "    for i, doc in enumerate(tqdm(dataset['documents'])):\n",
    "        new_document = {'metadata': {}}\n",
    "        \n",
    "        metadata = doc['meta'].copy()\n",
    "        metadata['filename'] = metadata['filepath'].split('/')[-1]\n",
    "        del metadata['filepath']\n",
    "\n",
    "        page_list = metadata['pages']\n",
    "        page_list = [page + before_page for page in page_list]\n",
    "        del metadata['pages']\n",
    "\n",
    "        new_document['metadata'] = metadata\n",
    "        new_document['metadata']['page'] = page_list\n",
    "        # new_document['metadata']['doc_id'] = before_id + i + 1\n",
    "\n",
    "        chunks, table_chunk, chunk_id = chunk_by_token_range_with_sentence_overlap(doc, chunk_id)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            new_chunk = {'metadata': metadata.copy()}\n",
    "            new_chunk['metadata']['chunk_id'] = chunk['chunk_id']\n",
    "            new_chunk['page_content'] = chunk['page_content']\n",
    "            new_dataset.append(new_chunk)\n",
    "        \n",
    "        for table in table_chunk:\n",
    "            new_table = {'metadata': {}}\n",
    "            new_table['metadata']['chunk_id'] = table['chunk_id']\n",
    "            new_table['metadata']['doc_id'] = before_id + i + 1\n",
    "            new_table['metadata']['caption'] = table['caption']\n",
    "            new_table['page_content'] = table['table']\n",
    "            new_table_dataset.append(new_table)\n",
    "\n",
    "    return new_dataset, new_table_dataset, chunk_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 69.44it/s]\n",
      "100%|██████████| 81/81 [00:00<00:00, 697.43it/s]\n",
      "100%|██████████| 104/104 [00:00<00:00, 1383.62it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 1623.18it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 1631.27it/s]\n"
     ]
    }
   ],
   "source": [
    "new_dataset_list = []\n",
    "new_table_dataset_list = []\n",
    "before_page = 0\n",
    "before_id = 0\n",
    "chunk_id = 0\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    new_dataset, new_table_dataset, chunk_id = make_dataset_for_search(dataset, before_page, before_id, chunk_id)\n",
    "    new_dataset_list.append(new_dataset)\n",
    "    new_table_dataset_list.extend(new_table_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/document/역도/chunk_with_overlap.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(new_dataset_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# with open('../data/document/역도/table_chunk.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(new_table_dataset_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SportAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
