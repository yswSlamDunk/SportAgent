{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_open(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/document/역도\\역도의 이해_dataset.json\n",
      "../data/document/역도\\역도의 스포츠 과학적 원리_dataset.json\n",
      "../data/document/역도\\역도경기 기술의 구조와 훈련법_dataset.json\n",
      "../data/document/역도\\역도체력의 구조와 훈련법_dataset.json\n",
      "../data/document/역도\\역도 훈련프로그램 구성 및 지도안_dataset.json\n"
     ]
    }
   ],
   "source": [
    "base_path = '../data/document/역도'\n",
    "\n",
    "name_list = ['역도의 이해', '역도의 스포츠 과학적 원리', '역도경기 기술의 구조와 훈련법', '역도체력의 구조와 훈련법', '역도 훈련프로그램 구성 및 지도안']\n",
    "path_list = []\n",
    "\n",
    "suffix = '_dataset.json'\n",
    "for name in name_list:\n",
    "    path = os.path.join(base_path, name + suffix)\n",
    "    path_list.append(path)\n",
    "\n",
    "dataset_list = []\n",
    "for path in path_list:\n",
    "    print(path)\n",
    "    file = file_open(path)\n",
    "    dataset_list.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 전처리\n",
    "## 검색 전용 데이터 생성\n",
    "1. 불필요 정보 제거\n",
    "   - figure, chart, table의 hypotheticalQuestions 제거(?)\n",
    "   - filepaths 제거\n",
    "   - category 제거 \n",
    "2. 페이지 수정\n",
    "3. 개별 문서, 병합 문서 생성\n",
    "\n",
    "### 검색 전용 Document 구조\n",
    "- id: int\n",
    "- metadata: Dict\n",
    "  - filename: str\n",
    "  - page: List[int]\n",
    "- page_content: str\n",
    "- type: Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_for_search(dataset, before_page, before_id):\n",
    "    new_dataset = []\n",
    "    for i, doc in enumerate(tqdm(dataset['documents'])):\n",
    "        new_document = {'id': before_id + i + 1,\n",
    "                        'metadata': {},\n",
    "                        'page_content': ''}\n",
    "\n",
    "        metadata = doc['meta']\n",
    "        metadata['filename'] = metadata['filepath'].split('/')[-1]\n",
    "        del metadata['filepath']\n",
    "\n",
    "        page_list = metadata['pages']\n",
    "        page_list = [page + before_page for page in page_list]\n",
    "        del metadata['pages']\n",
    "\n",
    "        new_document['metadata'] = metadata\n",
    "        new_document['metadata']['page'] = page_list\n",
    "\n",
    "        page_content = ''\n",
    "        for element in doc['content']:\n",
    "            if element['category'] == 'table':\n",
    "                json = element['information']['table_json'] # 문자열\n",
    "                page_content += json + '\\n'\n",
    "\n",
    "            if element['category'] in ['figure', 'chart', 'table']:\n",
    "                if 'caption' in element['information']:\n",
    "                    caption = element['information']['caption']\n",
    "                    page_content += caption + '\\n'\n",
    "                detail = element['information']['detail']\n",
    "                hypotheticalQuestions = element['information']['hypotheticalQuestions']\n",
    "                hypotheticalQuestions = ' '.join([question for question in hypotheticalQuestions])\n",
    "                page_content += detail + '\\n'\n",
    "                page_content += hypotheticalQuestions + '\\n'\n",
    "            \n",
    "            if element['category'] == 'paragraph':\n",
    "                page_content += element['information'] + '\\n'\n",
    "        \n",
    "        new_document['page_content'] = page_content\n",
    "        new_dataset.append(new_document)\n",
    "    \n",
    "    before_page = page_list[-1]\n",
    "    print(before_page)\n",
    "    before_id = before_id + len(dataset['documents'])\n",
    "    return new_dataset, before_page, before_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 63370.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 81511.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 82349.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_dataset_list = []\n",
    "before_page = 0\n",
    "before_id = 0\n",
    "\n",
    "for i, dataset in enumerate(dataset_list):\n",
    "    new_dataset, before_page, before_id = make_dataset_for_search(dataset, before_page, before_id)\n",
    "    new_dataset_list.append(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = []\n",
    "for i, dataset in enumerate(new_dataset_list):\n",
    "    merged_dataset.extend(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset = {'new_dataset': new_dataset_list, 'merged_dataset': merged_dataset}\n",
    "# with open('../data/document/역도/save_dataset.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(save_dataset, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS 실험\n",
    "### 평가 항목\n",
    "1. Generation \n",
    "   1. Faithfulness\n",
    "      * 주어진 문맥에 대한 생성된 답변의 사실적 일관성 측정\n",
    "      * 답변과 검색된 문맥(retrieved context)를 기준으로 계산\n",
    "      * (0, 1) 범위 스케일이며, 값이 높을수록 좋음\n",
    "      * 생성된 답변이 신뢰할 수 있다고(faithful) 간구되려면 답변에서 제시된 모든 주장이 주어진 문맥(given context)에서 추론될 수 있어야 함\n",
    "      * 생성된 답변에서 주장의 집합(claims)를 식별 -> 각 주장마다 주어진 맥락 기반 여부 확인\n",
    "      * 점수: context 기반의 답변 내 주장 수 / 전체 주장 수\n",
    "      * 예시\n",
    "        * 아이슈타인의 출생일자와 출생지는 어디인가?\n",
    "          * 답변 1: 아이슈타인은 독엘에서 1879/3/14에 태어났습니다.\n",
    "            * \n",
    "          * 답변 2: 아이슈타인은 독엘에서 1879/4/14에 태어났습니다.\n",
    "            * \n",
    "          * context: \n",
    "* \n",
    "   1. \n",
    "1. Retriever\n",
    "   1. \n",
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SportAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
