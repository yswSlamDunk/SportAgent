{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.prompts import load_prompt\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain, StuffDocumentsChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험설계\n",
    "- heading 기반의 chunking 방법론으로 인해, chunk별 paragraph와 글자 수의 편차가 존재함\n",
    "- 이러한 상황 속, 최적의 summarization technique 방법이 있을 수 있다고 가정\n",
    "- 동일 chunking들에 대해서 여러 technique를 적용 후 요약 평가\n",
    "\n",
    "### 실험결과: stuff 방법이 적합\n",
    "- heading 기반의 chunking 방법으로 인해, chunking 내에 paragraph는 유사 내용으로 구성됨\n",
    "- map-reduce, map-refine은 긴 문서를 효율적으로 요약하는 기법으로, chunking들의 크기에는 부적합\n",
    "- 또한 긴 chunking에서도 좋은 성능을 보이지 못함(요약 결과 및 비용)\n",
    "\n",
    "\n",
    "참고자료: https://discuss.pytorch.kr/t/gn-1-llm/4609?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "none_paragraph = []\n",
    "docs_with_metadata = []\n",
    "\n",
    "for i, document in enumerate(state['documents']):\n",
    "    heading_text = ' '.join([h for h in document['meta']['heading'].values() if h])\n",
    "    \n",
    "    paragraph_text = ' '.join([\n",
    "        content['information'] if isinstance(content['information'], str) \n",
    "        else content['information'].get('detail', '') \n",
    "        for content in document['content'] \n",
    "        if content['category'] == 'paragraph'\n",
    "    ])\n",
    "    \n",
    "    if paragraph_text == '':\n",
    "        none_paragraph.append(i)\n",
    "    \n",
    "    combined_text = heading_text + '\\n' + paragraph_text\n",
    "    texts.append(combined_text)\n",
    "    \n",
    "    metadata = {\n",
    "        'document_index': i,\n",
    "        'heading': heading_text,\n",
    "        'has_paragraph': paragraph_text != '',\n",
    "        'char_count': len(combined_text),\n",
    "        'paragraph_count': sum(1 for content in document['content'] if content['category'] == 'paragraph')\n",
    "    }\n",
    "    \n",
    "    doc = Document(page_content=combined_text, metadata=metadata)\n",
    "    docs_with_metadata.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt = load_prompt(\"../prompt/summary/map_20250225_01.yaml\")\n",
    "reduce_prompt = load_prompt(\"../prompt/summary/reduce_20250225_01.yaml\")\n",
    "refine_prompt = load_prompt(\"../prompt/summary/refine_20250225_01.yaml\")\n",
    "evaluation_prompt = load_prompt(\"../prompt/summary/summarization_evaluation_20250225_02.yaml\")\n",
    "stuff_prompt = load_prompt(\"../prompt/summary/stuff_20250317_01.yaml\")\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    import tiktoken\n",
    "    encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "def apply_stuff(doc):\n",
    "    prompt = stuff_prompt.format(document=doc.page_content)\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    tokens_input = count_tokens(prompt)\n",
    "    tokens_output = count_tokens(response.content)\n",
    "    \n",
    "    return {\n",
    "        'document_index': doc.metadata['document_index'],\n",
    "        'summary_technique': 'stuff',\n",
    "        'summary_text': response.content,\n",
    "        'token_total': tokens_input + tokens_output,\n",
    "        'token_input': tokens_input,\n",
    "        'token_output': tokens_output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_map_reduce(doc):\n",
    "    map_prompt_filled = map_prompt.format(heading=doc.metadata['heading'], documents=doc.page_content)\n",
    "    map_response = llm.invoke(map_prompt_filled)\n",
    "    map_result = map_response.content\n",
    "    \n",
    "    reduce_prompt_filled = reduce_prompt.format(previous_summary=\"\", new_context=map_result)\n",
    "    reduce_response = llm.invoke(reduce_prompt_filled)\n",
    "    reduce_result = reduce_response.content\n",
    "    \n",
    "    tokens_input = count_tokens(map_prompt_filled) + count_tokens(reduce_prompt_filled)\n",
    "    tokens_output = count_tokens(map_result) + count_tokens(reduce_result)\n",
    "    \n",
    "    return {\n",
    "        'document_index': doc.metadata['document_index'],\n",
    "        'summary_technique': 'map-reduce',\n",
    "        'summary_text': reduce_result,\n",
    "        'token_total': tokens_input + tokens_output,\n",
    "        'token_input': tokens_input,\n",
    "        'token_output': tokens_output,\n",
    "        'map_result': map_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_map_refine(doc):\n",
    "    map_prompt_filled = map_prompt.format(heading=doc.metadata['heading'], documents=doc.page_content)\n",
    "    map_response = llm.invoke(map_prompt_filled)\n",
    "    map_result = map_response.content\n",
    "    \n",
    "    refine_prompt_filled = refine_prompt.format(summaries=map_result)\n",
    "    refine_response = llm.invoke(refine_prompt_filled)\n",
    "    refine_result = refine_response.content\n",
    "    \n",
    "    tokens_input = count_tokens(map_prompt_filled) + count_tokens(refine_prompt_filled)\n",
    "    tokens_output = count_tokens(map_result) + count_tokens(refine_result)\n",
    "    \n",
    "    return {\n",
    "        'document_index': doc.metadata['document_index'],\n",
    "        'summary_technique': 'map-refine',\n",
    "        'summary_text': refine_result,\n",
    "        'token_total': tokens_input + tokens_output,\n",
    "        'token_input': tokens_input, \n",
    "        'token_output': tokens_output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results = []\n",
    "map_results = []\n",
    "\n",
    "selected_docs = [doc for doc in docs_with_metadata if doc.metadata['document_index'] not in [0, 1, 3, 7, 10]]\n",
    "\n",
    "for doc in tqdm(selected_docs):\n",
    "    stuff_result = apply_stuff(doc)\n",
    "    summary_results.append(stuff_result)\n",
    "    \n",
    "    map_reduce_result = apply_map_reduce(doc)\n",
    "    summary_results.append(map_reduce_result)\n",
    "    map_results.append({\n",
    "        'document_index': map_reduce_result['document_index'],\n",
    "        'map_result': map_reduce_result['map_result'],\n",
    "        'token_total': count_tokens(map_reduce_result['map_result']) + count_tokens(map_prompt.format(heading=doc.metadata['heading'], documents=doc.page_content)),\n",
    "        'token_input': count_tokens(map_prompt.format(heading=doc.metadata['heading'], documents=doc.page_content)),\n",
    "        'token_output': count_tokens(map_reduce_result['map_result'])\n",
    "    })\n",
    "    \n",
    "    map_refine_result = apply_map_refine(doc)\n",
    "    summary_results.append(map_refine_result)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "map_df = pd.DataFrame(map_results)\n",
    "\n",
    "summary_df.to_csv('../data/experiment/summary/summary_df.csv', index=False)\n",
    "map_df.to_csv('../data/experiment/summary/map_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_analysis = summary_df.join(\n",
    "    pd.DataFrame(\n",
    "        [(d.metadata['document_index'], d.metadata['has_paragraph'], d.metadata['paragraph_count'], d.metadata['char_count']) \n",
    "         for d in docs_with_metadata],\n",
    "        columns=['document_index', 'has_paragraph', 'paragraph_count', 'char_count']\n",
    "    ).set_index('document_index'),\n",
    "    on='document_index'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "techniques = summary_df['summary_technique'].unique()\n",
    "for technique in techniques:\n",
    "    subset = summary_df[summary_df['summary_technique'] == technique]\n",
    "    plt.scatter(subset['token_input'], subset['token_output'], label=technique, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Input Tokens')\n",
    "plt.ylabel('Output Tokens')\n",
    "plt.title('Comparison of Token Usage by Summary Technique')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../data/experiment/summary/token_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for technique in techniques:\n",
    "    subset = paragraph_analysis[paragraph_analysis['summary_technique'] == technique]\n",
    "    plt.scatter(subset['paragraph_count'], subset['token_output'], label=technique, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Paragraph Count')\n",
    "plt.ylabel('Output Tokens')\n",
    "plt.title('Summary Output Size by Paragraph Count')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../data/experiment/summary/paragraph_performance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary(original_doc, summary, format_str=\"1-6점\"):\n",
    "    eval_prompt = evaluation_prompt.format(\n",
    "        original_document=original_doc.page_content,\n",
    "        summary=summary,\n",
    "        format=format_str\n",
    "    )\n",
    "    \n",
    "    evaluation = llm.invoke(eval_prompt)\n",
    "    return evaluation.content\n",
    "\n",
    "evaluation_results = []\n",
    "sample_for_eval = summary_df.sample(n=min(10, len(summary_df)))\n",
    "\n",
    "for _, row in sample_for_eval.iterrows():\n",
    "    doc_idx = row['document_index']\n",
    "    original_doc = next(d for d in docs_with_metadata if d.metadata['document_index'] == doc_idx)\n",
    "    \n",
    "    eval_result = evaluate_summary(original_doc, row['summary_text'])\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        'document_index': doc_idx,\n",
    "        'summary_technique': row['summary_technique'],\n",
    "        'evaluation': eval_result\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "eval_df.to_csv('../data/experiment/summary/evaluation_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SportAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
